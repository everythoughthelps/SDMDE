# Copyright (C) 2019 Jin Han Lee
#
# This file is a part of BTS.
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>
import copy

import numpy as np
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader,Sampler
import torch.utils.data.distributed
from torchvision import transforms
from PIL import Image
import os
import random

from distributed_sampler_no_evenly_divisible import *


def _is_pil_image(img):
	return isinstance(img, Image.Image)


def _is_numpy_image(img):
	return isinstance(img, np.ndarray) and (img.ndim in {2, 3})


def preprocessing_transforms(mode):
	return transforms.Compose([
		ToTensor(mode=mode),
		pad(mode=mode)
	])


class BtsDataLoader(object):
	def __init__(self, args, mode,eval_data):
		if mode == 'train':
			self.training_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode))
			if args.distributed:
				self.train_sampler = PairBatchSampler(self.training_samples,args.batch_size)
				#self.train_sampler = torch.utils.data.distributed.DistributedSampler(self.training_samples)
			else:
				self.train_sampler = PairBatchSampler(self.training_samples,args.batch_size)

			self.data = DataLoader(self.training_samples,
								   num_workers=args.num_threads,
								   pin_memory=True,
								   batch_sampler=self.train_sampler)

		elif mode == 'online_eval':
			self.testing_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode),eval_dataset=eval_data)
			if args.distributed:
				# self.eval_sampler = torch.utils.data.distributed.DistributedSampler(self.testing_samples, shuffle=False)
				self.eval_sampler = DistributedSamplerNoEvenlyDivisible(self.testing_samples, shuffle=False)
			else:
				self.eval_sampler = None
			self.data = DataLoader(self.testing_samples, 1,
								   shuffle=False,
								   num_workers=1,
								   pin_memory=True,
								   sampler=self.eval_sampler)

		elif mode == 'test':
			self.testing_samples = DataLoadPreprocess(args, mode, transform=preprocessing_transforms(mode),eval_dataset=eval_data)
			self.data = DataLoader(self.testing_samples, 1, shuffle=False, num_workers=1)

		else:
			print('mode should be one of \'train, test, online_eval\'. Got {}'.format(mode))


class DataLoadPreprocess(Dataset):
	def __init__(self, args, mode, transform=None, is_for_online_eval=False,eval_dataset=None):
		self.args = args
		self.eval_dataset = eval_dataset
		if mode == 'online_eval' :
			if self.eval_dataset == 'kitti':
				with open(args.kitti_filenames_file_eval, 'r') as f:
					self.filenames = f.readlines()
			if self.eval_dataset == 'nyu':
				with open(args.nyu_filenames_file_eval, 'r') as f:
					self.filenames = f.readlines()

		elif mode == 'test':
			if self.eval_dataset == 'kitti':
				with open(args.kitti_filenames_file, 'r') as f:
					self.filenames = f.readlines()

			if self.eval_dataset == 'nyu':
				with open(args.nyu_filenames_file, 'r') as f:
					self.filenames = f.readlines()
		else:
			if args.kitti_filenames_file:
				with open(args.kitti_filenames_file, 'r') as f:
					self.kitti_filenames = f.readlines()
					self.filenames = self.kitti_filenames
			if args.nyu_filenames_file:
				with open(args.nyu_filenames_file, 'r') as f:
					self.nyu_filenames = f.readlines()
					self.filenames = self.nyu_filenames
			if args.kitti_filenames_file and args.nyu_filenames_file:
				with open(args.kitti_filenames_file, 'r') as f:
					self.kitti_filenames = f.readlines()
				with open(args.nyu_filenames_file, 'r') as f:
					self.nyu_filenames = f.readlines()
				self.filenames = self.kitti_filenames + self.nyu_filenames

		self.mode = mode
		self.transform = transform
		self.is_for_online_eval = is_for_online_eval

	def __getitem__(self, idx):
		global maxdepth
		sample_path = self.filenames[idx]
		focal = float(sample_path.split()[2])

		if self.mode == 'train':
			if 'drive' in  sample_path:
				image_path = os.path.join(self.args.kitti_data_path, "./" + sample_path.split()[0])
				depth_path = os.path.join(self.args.kitti_gt_path, "./" + sample_path.split()[1])

				image = Image.open(image_path)
				depth_gt = Image.open(depth_path)

				if self.args.do_kb_crop is True:
					height = image.height
					width = image.width
					top_margin = int(height - 352)
					left_margin = int((width - 1216) / 2)
					depth_gt = depth_gt.crop((left_margin, top_margin, left_margin + 1216, top_margin + 352))
					image = image.crop((left_margin, top_margin, left_margin + 1216, top_margin + 352))

				if self.args.do_random_rotate is True:
					random_angle = (random.random() - 0.5) * 2 * self.args.degree
					image = self.rotate_image(image, random_angle)
					depth_gt = self.rotate_image(depth_gt, random_angle, flag=Image.NEAREST)

				image = np.asarray(image, dtype=np.float32) / 255.0
				depth_gt = np.asarray(depth_gt, dtype=np.float32)
				depth_gt = np.expand_dims(depth_gt, axis=2)

				depth_gt = depth_gt / 256.0
				image, depth_gt = self.random_crop(image, depth_gt, self.args.kitti_input_height, self.args.kitti_input_width)
				image, depth_gt = self.train_preprocess(image, depth_gt)

				maxdepth = 80
				data = 'kitti'

			else:
				image_path = os.path.join(self.args.nyu_data_path, "./" + sample_path.split()[0])
				depth_path = os.path.join(self.args.nyu_gt_path, "./" + sample_path.split()[1])

				image = Image.open(image_path)
				depth_gt = Image.open(depth_path)


				# To avoid blank boundaries due to pixel registration
				depth_gt = depth_gt.crop((43, 45, 608, 472))
				image = image.crop((43, 45, 608, 472))

				if self.args.do_random_rotate is True:
					random_angle = (random.random() - 0.5) * 2 * self.args.degree
					image = self.rotate_image(image, random_angle)
					depth_gt = self.rotate_image(depth_gt, random_angle, flag=Image.NEAREST)

				image = np.asarray(image, dtype=np.float32) / 255.0
				depth_gt = np.asarray(depth_gt, dtype=np.float32)
				depth_gt = np.expand_dims(depth_gt, axis=2)

				depth_gt = depth_gt / 1000.0
				image, depth_gt = self.random_crop(image, depth_gt, self.args.nyu_input_height, self.args.nyu_input_width)

				image, depth_gt = self.train_preprocess(image, depth_gt)
				maxdepth = 10
				data = 'nyu'

			sample = {'image': image, 'depth': depth_gt, 'focal': focal, 'data':data}

		else:
			if self.mode == 'online_eval':
				if self.eval_dataset == 'kitti':
					data_path = self.args.kitti_data_path_eval
					maxdepth = 80
				if self.eval_dataset == 'nyu':
					data_path = self.args.nyu_data_path_eval
					maxdepth = 10

			else:
				data_path = self.args.data_path
				if self.args.dataset == 'kitti':
					maxdepth = 80
				if self.args.dataset == 'nyu':
					maxdepth = 10

			image_path = os.path.join(data_path, "./" + sample_path.split()[0])
			image = np.asarray(Image.open(image_path), dtype=np.float32) / 255.0

			if self.mode == 'online_eval':
				if self.eval_dataset == 'kitti':
					gt_path = self.args.kitti_gt_path_eval
				if self.eval_dataset == 'nyu':
					gt_path = self.args.nyu_gt_path_eval
				depth_path = os.path.join(gt_path, "./" + sample_path.split()[1])
				has_valid_depth = False
				try:
					depth_gt = Image.open(depth_path)
					has_valid_depth = True
				except IOError:
					depth_gt = False
					# print('Missing gt for {}'.format(image_path))

				if has_valid_depth:
					depth_gt = np.asarray(depth_gt, dtype=np.float32)
					depth_gt = np.expand_dims(depth_gt, axis=2)
					if self.eval_dataset == 'nyu':
						depth_gt = depth_gt / 1000.0
					else:
						depth_gt = depth_gt / 256.0

			else:
				image = self.centerCrop(image,[self.args.input_width,self.args.input_height])

			if self.args.do_kb_crop is True:
				height = image.shape[0]
				width = image.shape[1]
				top_margin = int(height - 352)
				left_margin = int((width - 1216) / 2)
				image = image[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]
				if self.mode == 'online_eval' and has_valid_depth:
					depth_gt = depth_gt[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]

			if self.mode == 'online_eval':
				sample = {'image': image, 'depth': depth_gt, 'focal': focal, 'has_valid_depth': has_valid_depth}
			else:
				sample = {'image': image, 'focal': focal}
		if self.transform:
			sample = self.transform(sample)
		sample['maxdepth'] = maxdepth
		return sample

	def get_class(self, id):
		data_name = self.filenames[id]
		data_class = 'drive' if 'drive' in data_name else 'nyu'
		return data_class

	def rotate_image(self, image, angle, flag=Image.BILINEAR):
		result = image.rotate(angle, resample=flag)
		return result

	def random_crop(self, img, depth, height, width):
		assert img.shape[0] >= height
		assert img.shape[1] >= width
		assert img.shape[0] == depth.shape[0]
		assert img.shape[1] == depth.shape[1]
		x = random.randint(0, img.shape[1] - width)
		y = random.randint(0, img.shape[0] - height)
		img = img[y:y + height, x:x + width, :]
		depth = depth[y:y + height, x:x + width, :]
		return img, depth

	def train_preprocess(self, image, depth_gt):
		# Random flipping
		do_flip = random.random()
		if do_flip > 0.5:
			image = (image[:, ::-1, :]).copy()
			depth_gt = (depth_gt[:, ::-1, :]).copy()

		# Random gamma, brightness, color augmentation
		do_augment = random.random()
		if do_augment > 0.5:
			image = self.augment_image(image)

		return image, depth_gt

	def augment_image(self, image):
		# gamma augmentation
		gamma = random.uniform(0.9, 1.1)
		image_aug = image ** gamma

		# brightness augmentation
		if image.shape == (416,544):
			brightness = random.uniform(0.75, 1.25)
		else:
			brightness = random.uniform(0.9, 1.1)
		image_aug = image_aug * brightness

		# color augmentation
		colors = np.random.uniform(0.9, 1.1, size=3)
		white = np.ones((image.shape[0], image.shape[1]))
		color_image = np.stack([white * colors[i] for i in range(3)], axis=2)
		image_aug *= color_image
		image_aug = np.clip(image_aug, 0, 1)

		return image_aug

	def __len__(self):
		return len(self.filenames)

	def centerCrop(self, image, size):
		w1, h1 = image.shape[1],image.shape[0]

		tw, th = size

		if w1 == tw and h1 == th:
			return image

		x1 = int(round((w1 - tw) / 2.))
		y1 = int(round((h1 - th) / 2.))

		image = image[y1:th+y1,x1:tw+x1,:]

		return image


class ToTensor(object):
	def __init__(self, mode):
		self.mode = mode
		self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

	def __call__(self, sample):
		image, focal, = sample['image'], sample['focal']
		image = self.to_tensor(image)
		image = self.normalize(image)

		if self.mode == 'test':
			return {'image': image, 'focal': focal }

		if self.mode == 'train':
			depth, data = sample['depth'], sample['data']
			depth = self.to_tensor(depth)
			return {'image': image, 'depth': depth, 'focal': focal,'data':data}
		else:
			depth = sample['depth']
			has_valid_depth = sample['has_valid_depth']
			return {'image': image, 'depth': depth, 'focal': focal, 'has_valid_depth': has_valid_depth}

	def to_tensor(self, pic):
		if not (_is_pil_image(pic) or _is_numpy_image(pic)):
			raise TypeError(
				'pic should be PIL Image or ndarray. Got {}'.format(type(pic)))

		if isinstance(pic, np.ndarray):
			img = torch.from_numpy(pic.transpose((2, 0, 1)))
			return img

		# handle PIL Image
		if pic.mode == 'I':
			img = torch.from_numpy(np.array(pic, np.int32, copy=False))
		elif pic.mode == 'I;16':
			img = torch.from_numpy(np.array(pic, np.int16, copy=False))
		else:
			img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))
		# PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK
		if pic.mode == 'YCbCr':
			nchannel = 3
		elif pic.mode == 'I;16':
			nchannel = 1
		else:
			nchannel = len(pic.mode)
		img = img.view(pic.size[1], pic.size[0], nchannel)

		img = img.transpose(0, 1).transpose(0, 2).contiguous()
		if isinstance(img, torch.ByteTensor):
			return img.float()
		else:
			return img

class pad(object):
	def __init__(self,mode):
		self.mode = mode

	def __call__(self, sample):
		if self.mode == 'train':
			data, image, depth, focal = sample['data'], sample['image'], sample['depth'], sample['focal']
			if data == 'kitti':
				zero_padding = nn.ZeroPad2d(padding=(0,0,64,0))
				image = zero_padding(image)
				depth = zero_padding(depth)
				return {'image': image, 'depth': depth,'focal':focal,'data':data}
			else:
				zero_padding = nn.ZeroPad2d(padding=(160,0,0,0))
				image = zero_padding(image)
				depth = zero_padding(depth)
				return {'image': image, 'depth': depth,'focal':focal,'data':data}
		if self.mode == 'test':
			image, focal = sample['image'], sample['focal']
			return {'image':image, 'focal':focal}

		else:
			image, depth, focal = sample['image'], sample['depth'], sample['focal']
			has_valid_depth = sample['has_valid_depth']
			return {'image': image, 'depth': depth, 'focal': focal, 'has_valid_depth': has_valid_depth}


class PairBatchSampler(object):
	def __init__(self, dataset, batch_size, num_replicas=None, rank=None):
		if num_replicas is None:
			if not dist.is_available():
				raise RuntimeError("Requires distributed package to be available")
			num_replicas = dist.get_world_size()
		if rank is None:
			if not dist.is_available():
				raise RuntimeError("Requires distributed package to be available")
			rank = dist.get_rank()
		self.dataset = dataset
		self.num_replicas = num_replicas
		self.rank = rank
		self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))
		self.total_size = self.num_samples * self.num_replicas
		self.nyu_dataset_filenames = dataset.nyu_filenames
		self.kitti_dataset_filenames = dataset.kitti_filenames
		self.batch_size = batch_size
		self.indices = list(range(len(self.dataset)))

		self.kitti_indices = self.indices[:len(self.kitti_dataset_filenames)]
		self.len_kitti = math.ceil(len(self.kitti_indices) / (self.batch_size * self.num_replicas))
		self.total_kitti_size = self.len_kitti * (self.batch_size * self.num_replicas)
		self.kitti_indices += self.kitti_indices[:(self.total_kitti_size - len(self.kitti_indices))]

		self.nyu_indices = self.indices[len(self.kitti_dataset_filenames):]
		self.len_nyu = math.ceil(len(self.nyu_indices) / (self.batch_size * self.num_replicas))
		self.total_nyu_size = self.len_nyu * (self.batch_size * self.num_replicas)
		self.nyu_indices += self.nyu_indices[:(self.total_nyu_size - len(self.nyu_indices))]

		random.shuffle(self.nyu_indices)
		random.shuffle(self.kitti_indices)

	def __iter__(self):

		g = torch.Generator()
		g.manual_seed(self.epoch)
		kitti_indices = self.kitti_indices[self.rank:len(self.kitti_indices):self.num_replicas]
		nyu_indices = self.nyu_indices[self.rank:len(self.nyu_indices):self.num_replicas]
		while nyu_indices or kitti_indices:
			oppo_indices = []
			sync_indices = []
			if torch.rand(1,generator=g) > 0.5:
				if nyu_indices:
					nyu_batch_indices = nyu_indices[:self.batch_size]
					nyu_indices = nyu_indices[self.batch_size:]
					for idx in nyu_batch_indices:
						oppo_index = random.choice(self.kitti_indices)
						oppo_indices.append(oppo_index)

						sync_index = random.choice(self.nyu_indices)
						sync_indices.append(sync_index)

					yield nyu_batch_indices + oppo_indices
				else:
					continue
			else:
				if kitti_indices:
					kitti_batch_indices = kitti_indices[:self.batch_size]
					kitti_indices = kitti_indices[self.batch_size:]
					for idx in kitti_batch_indices:
						oppo_index = random.choice(self.nyu_indices)
						oppo_indices.append(oppo_index)

						sync_index = random.choice(self.kitti_indices)
						sync_indices.append(sync_index)

					yield kitti_batch_indices + oppo_indices
				else:
					continue

	def __len__(self):
		return self.len_kitti + self.len_nyu


	def set_epoch(self, epoch):
		self.epoch = epoch
